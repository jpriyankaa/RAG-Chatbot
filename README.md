# ğŸ¤– RAG Chatbot - Free, Local, Lightweight

A production-ready **Retrieval-Augmented Generation (RAG) chatbot** that runs 100% locally using free, open-source tools. Ask questions about your PDFs and documents with an AI assistant powered by Ollama and FAISS.

**Key Features:**
- âœ… 100% Local - No cloud APIs, no data leaves your machine
- âœ… Free - Uses only open-source software
- âœ… Lightweight - Works on standard laptops (8GB+ RAM)
- âœ… Simple UI - Built with Streamlit
- âœ… Multiple Models - Choose between Mistral, Neural Chat, or Llama2
- âœ… Source Attribution - See which documents the answer came from

---

## ğŸš€ Quick Start (5 minutes)

### 1ï¸âƒ£ Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Install Ollama
1. Download from: https://ollama.ai
2. Install and run the application

### 3ï¸âƒ£ Pull a Model
```bash
# Mistral 7B (recommended - fastest & good quality)
ollama pull mistral

# Alternatives:
ollama pull neural-chat    # Chat-optimized
ollama pull llama2         # Highest quality (but slower)
```

### 4ï¸âƒ£ Start the Chatbot
```bash
streamlit run app.py
```

Open `http://localhost:8501` in your browser

### 5ï¸âƒ£ Use the Chatbot
1. Upload PDFs or text files
2. Click "Process Files"
3. Ask questions in the chat box
4. Get answers with source documents!

---

## ğŸ“Š What's Inside

```
rag-chatbot/
â”œâ”€â”€ app.py              # Main Streamlit application
â”œâ”€â”€ config.py           # Advanced configuration & tuning
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ SETUP.md           # Installation & setup guide
â”œâ”€â”€ ADVANCED.md        # Advanced features & optimization
â””â”€â”€ README.md          # This file
```

---

## ğŸ¯ How It Works

```
1. Upload Documents (PDF/TXT)
   â†“
2. Split into chunks (500 chars each)
   â†“
3. Convert to embeddings (Sentence Transformers)
   â†“
4. Store in FAISS vector database
   â†“
5. User asks question
   â†“
6. Find 3 most similar chunks
   â†“
7. Send to LLM (Ollama)
   â†“
8. Generate answer based on context
   â†“
9. Show answer + source documents
```

---

## ğŸ’» System Requirements

| Requirement | Minimum | Recommended |
|------------|---------|------------|
| RAM | 8GB | 16GB |
| Disk | 5GB | 10GB |
| Processor | Any | Modern CPU/GPU |
| Internet | For download only | For download only |

---

## ğŸ› ï¸ Configuration

### Easy Options (in UI)
- Model Selection (Mistral, Neural Chat, Llama2)
- Temperature (0 = focused, 1 = creative)
- Embedding Model (MiniLM/MPNET/Multilingual)
- Retrieval Count (how many docs to use)

### Advanced Options (in `config.py`)
- Chunk size and overlap
- Custom prompts
- Memory optimization
- Performance tuning

See `ADVANCED.md` for detailed configuration guide.

---

## ğŸ“ Model Comparison

| Model | Speed | Quality | VRAM | Best For |
|-------|-------|---------|------|----------|
| **Mistral 7B** â­ | âš¡âš¡âš¡ Fast | Very Good | 4GB | General use |
| Neural Chat | âš¡âš¡âš¡ Fast | Good | 4GB | Chat-focused |
| Llama2 | âš¡ Slow | Excellent | 7GB | High quality |

**Recommendation**: Start with **Mistral 7B** - best balance of speed and quality.

---

## ğŸ¨ Embedding Models

| Model | Speed | Quality | Best For |
|-------|-------|---------|----------|
| MiniLM-L6-v2 | âš¡âš¡âš¡ | Good | Quick demos |
| MPNET-base-v2 â­ | âš¡âš¡ | Very Good | General use |
| Distiluse-multilingual | âš¡âš¡ | Excellent | Multiple languages |

---

## ğŸ” Example Queries

```
ğŸ“š Research Paper:
"What are the main findings of this paper?"
"What methodology was used?"

ğŸ“– Book:
"Summarize chapter 3"
"What happens to the main character?"

ğŸ“„ Technical Docs:
"How do I set up the API?"
"What are the system requirements?"

âš–ï¸ Legal Document:
"What are my obligations?"
"What are the termination clauses?"
```

---

## âš™ï¸ Troubleshooting

### "Connection refused"
```bash
# Make sure Ollama is running
# Check if it's accessible: http://localhost:11434
```

### "Very slow responses"
1. Switch to Mistral model (faster)
2. Reduce retrieval count to 2
3. Use MiniLM embeddings
4. Check CPU/RAM usage

### "Model not found"
```bash
ollama list  # Check installed models
ollama pull mistral  # Download the model
```

See `SETUP.md` for complete troubleshooting guide.

---

## ğŸš€ Performance Tips

### For Speed (< 10 seconds per query)
```
Model: Mistral
Embedding: MiniLM-L6-v2
Chunk size: 400
Retrieved docs: 2
```

### For Quality (best answers)
```
Model: Llama2
Embedding: MPNET-base-v2
Chunk size: 600
Retrieved docs: 5
```

### For Low Memory (< 8GB)
```
Model: Mistral
Embedding: MiniLM-L6-v2
Chunk size: 300
Retrieved docs: 2
```

---

## ğŸ”’ Privacy & Security

âœ… **Fully Private** - All processing happens on your machine  
âœ… **No Tracking** - No analytics or telemetry  
âœ… **No Cloud** - Documents never leave your computer  
âœ… **Open Source** - Inspect the code yourself  
âœ… **Free** - No hidden costs or subscriptions  

---

## ğŸ“š What is RAG?

**RAG = Retrieval-Augmented Generation**

Instead of relying on the LLM's training data, RAG:
1. **Retrieves** relevant document chunks
2. **Augments** the prompt with this context
3. **Generates** an answer based on actual documents

**Why RAG is better:**
- âœ… Answers based on your actual documents
- âœ… Lower hallucination rate
- âœ… Can use smaller, faster models
- âœ… Works with private data
- âœ… Easy to update (just upload new docs)

---

## ğŸš¨ Common Mistakes

âŒ Not keeping Ollama running in background  
âŒ Uploading duplicates or irrelevant documents  
âŒ Using very large chunk sizes (>1000)  
âŒ Expecting 100% accuracy (RAG is ~85-95%)  
âŒ Not checking source documents  
âŒ Running on very old machines  

---

## ğŸ’¡ Pro Tips

1. **Experiment** - Try different models, find best for your docs
2. **Clean Documents** - Good input = good answers
3. **Chunk Overlap** - Prevents cutting important info
4. **Check Sources** - Always verify answers in source docs
5. **Custom Prompts** - Tailor for your specific domain

---

## ğŸ”— Resources

- **Ollama**: https://ollama.ai
- **LangChain**: https://python.langchain.com
- **Sentence Transformers**: https://www.sbert.net
- **FAISS**: https://faiss.ai

---

## ğŸ“ Next Steps

1. **Install** - Follow Quick Start above
2. **Test** - Upload a sample PDF
3. **Experiment** - Try different models
4. **Read** - Check `SETUP.md` and `ADVANCED.md`
5. **Optimize** - Use `config.py` to tune for your needs

---

## ğŸ¯ Use Cases

- ğŸ“š **Research** - Analyze papers, extract findings
- ğŸ“– **Reading** - Summarize books, find information
- ğŸ”§ **Technical** - Browse documentation, code examples
- âš–ï¸ **Legal** - Review contracts, understand clauses
- ğŸ“ **Education** - Study materials, practice questions
- ğŸ’¼ **Work** - Analyze reports, meeting notes

---

## ğŸ“ Help & Support

- Check `SETUP.md` for installation help
- Check `ADVANCED.md` for optimization tips
- Check `config.py` for detailed parameters
- Review error messages - they usually point to the solution

---

## ğŸ‰ Ready to Start?

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Start Ollama application
# (Download from https://ollama.ai)

# 3. Pull a model
ollama pull mistral

# 4. Run the chatbot
streamlit run app.py

# 5. Open http://localhost:8501
```

**That's it! Start chatting with your documents!** ğŸš€

---

## ğŸ“„ License

This project uses open-source libraries under their respective licenses:
- LangChain (MIT)
- Ollama (MIT)
- FAISS (MIT)
- Streamlit (Apache 2.0)
- Sentence Transformers (Apache 2.0)

---

**Made with â¤ï¸ using free, open-source tools**

*Happy chatting! ğŸ¤–*
